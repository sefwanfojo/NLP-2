{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
    "\n",
    "When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:\n",
    "\n",
    "1. **Most common words** - find these and create word clouds\n",
    "2. **Size of vocabulary** - look number of unique words and also how quickly someone speaks\n",
    "3. **Amount of profanity** - most common terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the document-term matrix\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ali': [('like', 126),\n",
       "  ('im', 74),\n",
       "  ('know', 65),\n",
       "  ('just', 64),\n",
       "  ('dont', 61),\n",
       "  ('shit', 34),\n",
       "  ('thats', 34),\n",
       "  ('youre', 31),\n",
       "  ('gonna', 28),\n",
       "  ('ok', 26),\n",
       "  ('lot', 24),\n",
       "  ('gotta', 21),\n",
       "  ('oh', 21),\n",
       "  ('wanna', 21),\n",
       "  ('husband', 20),\n",
       "  ('got', 19),\n",
       "  ('time', 19),\n",
       "  ('right', 19),\n",
       "  ('cause', 18),\n",
       "  ('women', 17),\n",
       "  ('day', 17),\n",
       "  ('people', 16),\n",
       "  ('pregnant', 15),\n",
       "  ('hes', 14),\n",
       "  ('need', 14),\n",
       "  ('god', 14),\n",
       "  ('yeah', 13),\n",
       "  ('tell', 13),\n",
       "  ('theyre', 12),\n",
       "  ('dude', 12)],\n",
       " 'anthony': [('im', 60),\n",
       "  ('like', 50),\n",
       "  ('know', 39),\n",
       "  ('dont', 38),\n",
       "  ('got', 34),\n",
       "  ('joke', 34),\n",
       "  ('thats', 31),\n",
       "  ('said', 31),\n",
       "  ('anthony', 27),\n",
       "  ('day', 26),\n",
       "  ('say', 26),\n",
       "  ('just', 26),\n",
       "  ('guys', 23),\n",
       "  ('people', 22),\n",
       "  ('tell', 19),\n",
       "  ('youre', 19),\n",
       "  ('grandma', 18),\n",
       "  ('right', 18),\n",
       "  ('time', 17),\n",
       "  ('think', 17),\n",
       "  ('thing', 17),\n",
       "  ('school', 16),\n",
       "  ('jokes', 16),\n",
       "  ('yeah', 16),\n",
       "  ('did', 16),\n",
       "  ('good', 16),\n",
       "  ('okay', 15),\n",
       "  ('gonna', 15),\n",
       "  ('ive', 15),\n",
       "  ('baby', 15)],\n",
       " 'bill': [('like', 200),\n",
       "  ('just', 149),\n",
       "  ('right', 131),\n",
       "  ('im', 107),\n",
       "  ('know', 99),\n",
       "  ('dont', 95),\n",
       "  ('gonna', 77),\n",
       "  ('got', 72),\n",
       "  ('fucking', 70),\n",
       "  ('yeah', 67),\n",
       "  ('shit', 63),\n",
       "  ('youre', 59),\n",
       "  ('thats', 56),\n",
       "  ('dude', 40),\n",
       "  ('think', 36),\n",
       "  ('fuck', 36),\n",
       "  ('want', 36),\n",
       "  ('people', 32),\n",
       "  ('did', 31),\n",
       "  ('hes', 31),\n",
       "  ('guy', 30),\n",
       "  ('didnt', 29),\n",
       "  ('make', 28),\n",
       "  ('come', 27),\n",
       "  ('thing', 26),\n",
       "  ('going', 26),\n",
       "  ('theyre', 25),\n",
       "  ('let', 24),\n",
       "  ('theres', 24),\n",
       "  ('doing', 23)],\n",
       " 'bo': [('know', 50),\n",
       "  ('like', 44),\n",
       "  ('think', 37),\n",
       "  ('love', 37),\n",
       "  ('im', 37),\n",
       "  ('bo', 35),\n",
       "  ('just', 35),\n",
       "  ('stuff', 33),\n",
       "  ('repeat', 31),\n",
       "  ('dont', 29),\n",
       "  ('yeah', 27),\n",
       "  ('want', 25),\n",
       "  ('right', 24),\n",
       "  ('cos', 23),\n",
       "  ('eye', 22),\n",
       "  ('said', 22),\n",
       "  ('people', 22),\n",
       "  ('fucking', 22),\n",
       "  ('um', 21),\n",
       "  ('prolonged', 21),\n",
       "  ('contact', 21),\n",
       "  ('thats', 19),\n",
       "  ('youre', 19),\n",
       "  ('time', 18),\n",
       "  ('little', 17),\n",
       "  ('sluts', 17),\n",
       "  ('man', 17),\n",
       "  ('good', 17),\n",
       "  ('brain', 15),\n",
       "  ('oh', 15)],\n",
       " 'dave': [('like', 103),\n",
       "  ('know', 79),\n",
       "  ('said', 63),\n",
       "  ('just', 61),\n",
       "  ('im', 47),\n",
       "  ('shit', 45),\n",
       "  ('people', 43),\n",
       "  ('didnt', 39),\n",
       "  ('ahah', 38),\n",
       "  ('dont', 38),\n",
       "  ('time', 36),\n",
       "  ('fuck', 33),\n",
       "  ('thats', 33),\n",
       "  ('fucking', 32),\n",
       "  ('black', 31),\n",
       "  ('man', 30),\n",
       "  ('got', 27),\n",
       "  ('good', 27),\n",
       "  ('right', 22),\n",
       "  ('gonna', 21),\n",
       "  ('lot', 20),\n",
       "  ('gay', 20),\n",
       "  ('nigga', 20),\n",
       "  ('hes', 19),\n",
       "  ('did', 19),\n",
       "  ('oh', 18),\n",
       "  ('yeah', 18),\n",
       "  ('oj', 18),\n",
       "  ('come', 17),\n",
       "  ('guys', 16)],\n",
       " 'hasan': [('like', 220),\n",
       "  ('im', 136),\n",
       "  ('know', 70),\n",
       "  ('dont', 64),\n",
       "  ('dad', 59),\n",
       "  ('youre', 51),\n",
       "  ('just', 46),\n",
       "  ('going', 41),\n",
       "  ('thats', 39),\n",
       "  ('want', 38),\n",
       "  ('got', 35),\n",
       "  ('love', 34),\n",
       "  ('shes', 32),\n",
       "  ('hasan', 31),\n",
       "  ('say', 30),\n",
       "  ('right', 30),\n",
       "  ('time', 27),\n",
       "  ('life', 25),\n",
       "  ('mom', 25),\n",
       "  ('people', 25),\n",
       "  ('oh', 24),\n",
       "  ('hey', 24),\n",
       "  ('look', 22),\n",
       "  ('did', 22),\n",
       "  ('brown', 21),\n",
       "  ('white', 20),\n",
       "  ('guys', 20),\n",
       "  ('parents', 20),\n",
       "  ('school', 19),\n",
       "  ('girl', 19)],\n",
       " 'jim': [('like', 108),\n",
       "  ('im', 101),\n",
       "  ('dont', 90),\n",
       "  ('right', 81),\n",
       "  ('fucking', 78),\n",
       "  ('just', 63),\n",
       "  ('went', 63),\n",
       "  ('know', 63),\n",
       "  ('youre', 48),\n",
       "  ('people', 44),\n",
       "  ('thats', 42),\n",
       "  ('day', 40),\n",
       "  ('oh', 40),\n",
       "  ('think', 39),\n",
       "  ('going', 39),\n",
       "  ('fuck', 37),\n",
       "  ('thing', 34),\n",
       "  ('goes', 34),\n",
       "  ('said', 32),\n",
       "  ('guns', 30),\n",
       "  ('theyre', 29),\n",
       "  ('good', 28),\n",
       "  ('ive', 27),\n",
       "  ('women', 26),\n",
       "  ('got', 26),\n",
       "  ('cause', 26),\n",
       "  ('theres', 26),\n",
       "  ('want', 25),\n",
       "  ('hes', 23),\n",
       "  ('really', 23)],\n",
       " 'joe': [('like', 143),\n",
       "  ('people', 100),\n",
       "  ('just', 87),\n",
       "  ('dont', 79),\n",
       "  ('im', 69),\n",
       "  ('fucking', 69),\n",
       "  ('fuck', 66),\n",
       "  ('thats', 62),\n",
       "  ('gonna', 52),\n",
       "  ('theyre', 49),\n",
       "  ('know', 46),\n",
       "  ('youre', 42),\n",
       "  ('think', 41),\n",
       "  ('shit', 40),\n",
       "  ('got', 36),\n",
       "  ('theres', 34),\n",
       "  ('right', 31),\n",
       "  ('man', 30),\n",
       "  ('house', 27),\n",
       "  ('oh', 25),\n",
       "  ('kids', 25),\n",
       "  ('cause', 24),\n",
       "  ('white', 24),\n",
       "  ('say', 23),\n",
       "  ('real', 22),\n",
       "  ('life', 21),\n",
       "  ('time', 20),\n",
       "  ('dude', 20),\n",
       "  ('make', 20),\n",
       "  ('gotta', 20)],\n",
       " 'john': [('like', 190),\n",
       "  ('know', 66),\n",
       "  ('just', 53),\n",
       "  ('dont', 52),\n",
       "  ('said', 39),\n",
       "  ('clinton', 34),\n",
       "  ('im', 33),\n",
       "  ('thats', 31),\n",
       "  ('right', 29),\n",
       "  ('youre', 28),\n",
       "  ('little', 26),\n",
       "  ('hey', 25),\n",
       "  ('got', 24),\n",
       "  ('time', 24),\n",
       "  ('cause', 22),\n",
       "  ('people', 22),\n",
       "  ('say', 22),\n",
       "  ('mom', 22),\n",
       "  ('old', 21),\n",
       "  ('day', 21),\n",
       "  ('oh', 21),\n",
       "  ('way', 21),\n",
       "  ('gonna', 21),\n",
       "  ('think', 21),\n",
       "  ('cow', 20),\n",
       "  ('went', 18),\n",
       "  ('wife', 18),\n",
       "  ('really', 18),\n",
       "  ('real', 17),\n",
       "  ('dad', 17)],\n",
       " 'louis': [('like', 110),\n",
       "  ('just', 97),\n",
       "  ('know', 70),\n",
       "  ('dont', 53),\n",
       "  ('thats', 51),\n",
       "  ('im', 50),\n",
       "  ('youre', 50),\n",
       "  ('life', 41),\n",
       "  ('people', 40),\n",
       "  ('thing', 31),\n",
       "  ('gonna', 29),\n",
       "  ('hes', 29),\n",
       "  ('cause', 28),\n",
       "  ('theres', 28),\n",
       "  ('shit', 25),\n",
       "  ('time', 22),\n",
       "  ('good', 22),\n",
       "  ('tit', 22),\n",
       "  ('theyre', 21),\n",
       "  ('think', 21),\n",
       "  ('right', 21),\n",
       "  ('really', 20),\n",
       "  ('course', 19),\n",
       "  ('murder', 18),\n",
       "  ('guy', 18),\n",
       "  ('kids', 18),\n",
       "  ('ok', 17),\n",
       "  ('mean', 15),\n",
       "  ('fuck', 15),\n",
       "  ('old', 15)],\n",
       " 'mike': [('like', 234),\n",
       "  ('im', 142),\n",
       "  ('know', 105),\n",
       "  ('said', 88),\n",
       "  ('just', 83),\n",
       "  ('dont', 76),\n",
       "  ('think', 51),\n",
       "  ('thats', 51),\n",
       "  ('says', 46),\n",
       "  ('cause', 35),\n",
       "  ('right', 34),\n",
       "  ('jenny', 33),\n",
       "  ('goes', 32),\n",
       "  ('really', 30),\n",
       "  ('id', 30),\n",
       "  ('point', 28),\n",
       "  ('mean', 28),\n",
       "  ('youre', 28),\n",
       "  ('gonna', 27),\n",
       "  ('got', 25),\n",
       "  ('yeah', 25),\n",
       "  ('people', 23),\n",
       "  ('kind', 23),\n",
       "  ('uh', 22),\n",
       "  ('say', 21),\n",
       "  ('feel', 20),\n",
       "  ('want', 19),\n",
       "  ('didnt', 19),\n",
       "  ('going', 19),\n",
       "  ('time', 19)],\n",
       " 'ricky': [('right', 110),\n",
       "  ('like', 80),\n",
       "  ('im', 66),\n",
       "  ('just', 66),\n",
       "  ('dont', 56),\n",
       "  ('know', 55),\n",
       "  ('said', 51),\n",
       "  ('yeah', 49),\n",
       "  ('fucking', 47),\n",
       "  ('got', 44),\n",
       "  ('say', 43),\n",
       "  ('youre', 41),\n",
       "  ('went', 40),\n",
       "  ('id', 39),\n",
       "  ('thats', 38),\n",
       "  ('people', 34),\n",
       "  ('didnt', 33),\n",
       "  ('little', 32),\n",
       "  ('joke', 31),\n",
       "  ('theyre', 29),\n",
       "  ('hes', 29),\n",
       "  ('ive', 28),\n",
       "  ('going', 26),\n",
       "  ('thing', 26),\n",
       "  ('years', 24),\n",
       "  ('day', 23),\n",
       "  ('theres', 22),\n",
       "  ('saying', 22),\n",
       "  ('ill', 21),\n",
       "  ('big', 21)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 30 words said by each comedian\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ali\n",
      "like, im, know, just, dont, shit, thats, youre, gonna, ok, lot, gotta, oh, wanna\n",
      "---\n",
      "anthony\n",
      "im, like, know, dont, got, joke, thats, said, anthony, day, say, just, guys, people\n",
      "---\n",
      "bill\n",
      "like, just, right, im, know, dont, gonna, got, fucking, yeah, shit, youre, thats, dude\n",
      "---\n",
      "bo\n",
      "know, like, think, love, im, bo, just, stuff, repeat, dont, yeah, want, right, cos\n",
      "---\n",
      "dave\n",
      "like, know, said, just, im, shit, people, didnt, ahah, dont, time, fuck, thats, fucking\n",
      "---\n",
      "hasan\n",
      "like, im, know, dont, dad, youre, just, going, thats, want, got, love, shes, hasan\n",
      "---\n",
      "jim\n",
      "like, im, dont, right, fucking, just, went, know, youre, people, thats, day, oh, think\n",
      "---\n",
      "joe\n",
      "like, people, just, dont, im, fucking, fuck, thats, gonna, theyre, know, youre, think, shit\n",
      "---\n",
      "john\n",
      "like, know, just, dont, said, clinton, im, thats, right, youre, little, hey, got, time\n",
      "---\n",
      "louis\n",
      "like, just, know, dont, thats, im, youre, life, people, thing, gonna, hes, cause, theres\n",
      "---\n",
      "mike\n",
      "like, im, know, said, just, dont, think, thats, says, cause, right, jenny, goes, really\n",
      "---\n",
      "ricky\n",
      "right, like, im, just, dont, know, said, yeah, fucking, got, say, youre, went, id\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Print the top 15 words said by each comedian\n",
    "for comedian, top_words in top_dict.items():\n",
    "    print(comedian)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'im',\n",
       " 'know',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'shit',\n",
       " 'thats',\n",
       " 'youre',\n",
       " 'gonna',\n",
       " 'ok',\n",
       " 'lot',\n",
       " 'gotta',\n",
       " 'oh',\n",
       " 'wanna',\n",
       " 'husband',\n",
       " 'got',\n",
       " 'time',\n",
       " 'right',\n",
       " 'cause',\n",
       " 'women',\n",
       " 'day',\n",
       " 'people',\n",
       " 'pregnant',\n",
       " 'hes',\n",
       " 'need',\n",
       " 'god',\n",
       " 'yeah',\n",
       " 'tell',\n",
       " 'theyre',\n",
       " 'dude',\n",
       " 'im',\n",
       " 'like',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'got',\n",
       " 'joke',\n",
       " 'thats',\n",
       " 'said',\n",
       " 'anthony',\n",
       " 'day',\n",
       " 'say',\n",
       " 'just',\n",
       " 'guys',\n",
       " 'people',\n",
       " 'tell',\n",
       " 'youre',\n",
       " 'grandma',\n",
       " 'right',\n",
       " 'time',\n",
       " 'think',\n",
       " 'thing',\n",
       " 'school',\n",
       " 'jokes',\n",
       " 'yeah',\n",
       " 'did',\n",
       " 'good',\n",
       " 'okay',\n",
       " 'gonna',\n",
       " 'ive',\n",
       " 'baby',\n",
       " 'like',\n",
       " 'just',\n",
       " 'right',\n",
       " 'im',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'gonna',\n",
       " 'got',\n",
       " 'fucking',\n",
       " 'yeah',\n",
       " 'shit',\n",
       " 'youre',\n",
       " 'thats',\n",
       " 'dude',\n",
       " 'think',\n",
       " 'fuck',\n",
       " 'want',\n",
       " 'people',\n",
       " 'did',\n",
       " 'hes',\n",
       " 'guy',\n",
       " 'didnt',\n",
       " 'make',\n",
       " 'come',\n",
       " 'thing',\n",
       " 'going',\n",
       " 'theyre',\n",
       " 'let',\n",
       " 'theres',\n",
       " 'doing',\n",
       " 'know',\n",
       " 'like',\n",
       " 'think',\n",
       " 'love',\n",
       " 'im',\n",
       " 'bo',\n",
       " 'just',\n",
       " 'stuff',\n",
       " 'repeat',\n",
       " 'dont',\n",
       " 'yeah',\n",
       " 'want',\n",
       " 'right',\n",
       " 'cos',\n",
       " 'eye',\n",
       " 'said',\n",
       " 'people',\n",
       " 'fucking',\n",
       " 'um',\n",
       " 'prolonged',\n",
       " 'contact',\n",
       " 'thats',\n",
       " 'youre',\n",
       " 'time',\n",
       " 'little',\n",
       " 'sluts',\n",
       " 'man',\n",
       " 'good',\n",
       " 'brain',\n",
       " 'oh',\n",
       " 'like',\n",
       " 'know',\n",
       " 'said',\n",
       " 'just',\n",
       " 'im',\n",
       " 'shit',\n",
       " 'people',\n",
       " 'didnt',\n",
       " 'ahah',\n",
       " 'dont',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'thats',\n",
       " 'fucking',\n",
       " 'black',\n",
       " 'man',\n",
       " 'got',\n",
       " 'good',\n",
       " 'right',\n",
       " 'gonna',\n",
       " 'lot',\n",
       " 'gay',\n",
       " 'nigga',\n",
       " 'hes',\n",
       " 'did',\n",
       " 'oh',\n",
       " 'yeah',\n",
       " 'oj',\n",
       " 'come',\n",
       " 'guys',\n",
       " 'like',\n",
       " 'im',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'dad',\n",
       " 'youre',\n",
       " 'just',\n",
       " 'going',\n",
       " 'thats',\n",
       " 'want',\n",
       " 'got',\n",
       " 'love',\n",
       " 'shes',\n",
       " 'hasan',\n",
       " 'say',\n",
       " 'right',\n",
       " 'time',\n",
       " 'life',\n",
       " 'mom',\n",
       " 'people',\n",
       " 'oh',\n",
       " 'hey',\n",
       " 'look',\n",
       " 'did',\n",
       " 'brown',\n",
       " 'white',\n",
       " 'guys',\n",
       " 'parents',\n",
       " 'school',\n",
       " 'girl',\n",
       " 'like',\n",
       " 'im',\n",
       " 'dont',\n",
       " 'right',\n",
       " 'fucking',\n",
       " 'just',\n",
       " 'went',\n",
       " 'know',\n",
       " 'youre',\n",
       " 'people',\n",
       " 'thats',\n",
       " 'day',\n",
       " 'oh',\n",
       " 'think',\n",
       " 'going',\n",
       " 'fuck',\n",
       " 'thing',\n",
       " 'goes',\n",
       " 'said',\n",
       " 'guns',\n",
       " 'theyre',\n",
       " 'good',\n",
       " 'ive',\n",
       " 'women',\n",
       " 'got',\n",
       " 'cause',\n",
       " 'theres',\n",
       " 'want',\n",
       " 'hes',\n",
       " 'really',\n",
       " 'like',\n",
       " 'people',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'im',\n",
       " 'fucking',\n",
       " 'fuck',\n",
       " 'thats',\n",
       " 'gonna',\n",
       " 'theyre',\n",
       " 'know',\n",
       " 'youre',\n",
       " 'think',\n",
       " 'shit',\n",
       " 'got',\n",
       " 'theres',\n",
       " 'right',\n",
       " 'man',\n",
       " 'house',\n",
       " 'oh',\n",
       " 'kids',\n",
       " 'cause',\n",
       " 'white',\n",
       " 'say',\n",
       " 'real',\n",
       " 'life',\n",
       " 'time',\n",
       " 'dude',\n",
       " 'make',\n",
       " 'gotta',\n",
       " 'like',\n",
       " 'know',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'said',\n",
       " 'clinton',\n",
       " 'im',\n",
       " 'thats',\n",
       " 'right',\n",
       " 'youre',\n",
       " 'little',\n",
       " 'hey',\n",
       " 'got',\n",
       " 'time',\n",
       " 'cause',\n",
       " 'people',\n",
       " 'say',\n",
       " 'mom',\n",
       " 'old',\n",
       " 'day',\n",
       " 'oh',\n",
       " 'way',\n",
       " 'gonna',\n",
       " 'think',\n",
       " 'cow',\n",
       " 'went',\n",
       " 'wife',\n",
       " 'really',\n",
       " 'real',\n",
       " 'dad',\n",
       " 'like',\n",
       " 'just',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'thats',\n",
       " 'im',\n",
       " 'youre',\n",
       " 'life',\n",
       " 'people',\n",
       " 'thing',\n",
       " 'gonna',\n",
       " 'hes',\n",
       " 'cause',\n",
       " 'theres',\n",
       " 'shit',\n",
       " 'time',\n",
       " 'good',\n",
       " 'tit',\n",
       " 'theyre',\n",
       " 'think',\n",
       " 'right',\n",
       " 'really',\n",
       " 'course',\n",
       " 'murder',\n",
       " 'guy',\n",
       " 'kids',\n",
       " 'ok',\n",
       " 'mean',\n",
       " 'fuck',\n",
       " 'old',\n",
       " 'like',\n",
       " 'im',\n",
       " 'know',\n",
       " 'said',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'thats',\n",
       " 'says',\n",
       " 'cause',\n",
       " 'right',\n",
       " 'jenny',\n",
       " 'goes',\n",
       " 'really',\n",
       " 'id',\n",
       " 'point',\n",
       " 'mean',\n",
       " 'youre',\n",
       " 'gonna',\n",
       " 'got',\n",
       " 'yeah',\n",
       " 'people',\n",
       " 'kind',\n",
       " 'uh',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'want',\n",
       " 'didnt',\n",
       " 'going',\n",
       " 'time',\n",
       " 'right',\n",
       " 'like',\n",
       " 'im',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'fucking',\n",
       " 'got',\n",
       " 'say',\n",
       " 'youre',\n",
       " 'went',\n",
       " 'id',\n",
       " 'thats',\n",
       " 'people',\n",
       " 'didnt',\n",
       " 'little',\n",
       " 'joke',\n",
       " 'theyre',\n",
       " 'hes',\n",
       " 'ive',\n",
       " 'going',\n",
       " 'thing',\n",
       " 'years',\n",
       " 'day',\n",
       " 'theres',\n",
       " 'saying',\n",
       " 'ill',\n",
       " 'big']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each comedian\n",
    "words = []\n",
    "for comedian in data.columns:\n",
    "    top = [word for (word, count) in top_dict[comedian]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 12),\n",
       " ('im', 12),\n",
       " ('know', 12),\n",
       " ('just', 12),\n",
       " ('dont', 12),\n",
       " ('thats', 12),\n",
       " ('right', 12),\n",
       " ('people', 12),\n",
       " ('youre', 11),\n",
       " ('got', 10),\n",
       " ('time', 9),\n",
       " ('gonna', 8),\n",
       " ('think', 8),\n",
       " ('oh', 7),\n",
       " ('yeah', 7),\n",
       " ('said', 7),\n",
       " ('cause', 6),\n",
       " ('hes', 6),\n",
       " ('theyre', 6),\n",
       " ('say', 6),\n",
       " ('fucking', 6),\n",
       " ('shit', 5),\n",
       " ('day', 5),\n",
       " ('thing', 5),\n",
       " ('good', 5),\n",
       " ('fuck', 5),\n",
       " ('want', 5),\n",
       " ('going', 5),\n",
       " ('theres', 5),\n",
       " ('did', 4),\n",
       " ('didnt', 4),\n",
       " ('really', 4),\n",
       " ('dude', 3),\n",
       " ('guys', 3),\n",
       " ('ive', 3),\n",
       " ('little', 3),\n",
       " ('man', 3),\n",
       " ('life', 3),\n",
       " ('went', 3),\n",
       " ('ok', 2),\n",
       " ('lot', 2),\n",
       " ('gotta', 2),\n",
       " ('women', 2),\n",
       " ('tell', 2),\n",
       " ('joke', 2),\n",
       " ('school', 2),\n",
       " ('guy', 2),\n",
       " ('make', 2),\n",
       " ('come', 2),\n",
       " ('love', 2),\n",
       " ('dad', 2),\n",
       " ('mom', 2),\n",
       " ('hey', 2),\n",
       " ('white', 2),\n",
       " ('goes', 2),\n",
       " ('kids', 2),\n",
       " ('real', 2),\n",
       " ('old', 2),\n",
       " ('mean', 2),\n",
       " ('id', 2),\n",
       " ('wanna', 1),\n",
       " ('husband', 1),\n",
       " ('pregnant', 1),\n",
       " ('need', 1),\n",
       " ('god', 1),\n",
       " ('anthony', 1),\n",
       " ('grandma', 1),\n",
       " ('jokes', 1),\n",
       " ('okay', 1),\n",
       " ('baby', 1),\n",
       " ('let', 1),\n",
       " ('doing', 1),\n",
       " ('bo', 1),\n",
       " ('stuff', 1),\n",
       " ('repeat', 1),\n",
       " ('cos', 1),\n",
       " ('eye', 1),\n",
       " ('um', 1),\n",
       " ('prolonged', 1),\n",
       " ('contact', 1),\n",
       " ('sluts', 1),\n",
       " ('brain', 1),\n",
       " ('ahah', 1),\n",
       " ('black', 1),\n",
       " ('gay', 1),\n",
       " ('nigga', 1),\n",
       " ('oj', 1),\n",
       " ('shes', 1),\n",
       " ('hasan', 1),\n",
       " ('look', 1),\n",
       " ('brown', 1),\n",
       " ('parents', 1),\n",
       " ('girl', 1),\n",
       " ('guns', 1),\n",
       " ('house', 1),\n",
       " ('clinton', 1),\n",
       " ('way', 1),\n",
       " ('cow', 1),\n",
       " ('wife', 1),\n",
       " ('tit', 1),\n",
       " ('course', 1),\n",
       " ('murder', 1),\n",
       " ('says', 1),\n",
       " ('jenny', 1),\n",
       " ('point', 1),\n",
       " ('kind', 1),\n",
       " ('uh', 1),\n",
       " ('feel', 1),\n",
       " ('years', 1),\n",
       " ('saying', 1),\n",
       " ('ill', 1),\n",
       " ('big', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'im',\n",
       " 'know',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'thats',\n",
       " 'right',\n",
       " 'people',\n",
       " 'youre',\n",
       " 'got',\n",
       " 'time',\n",
       " 'gonna',\n",
       " 'think',\n",
       " 'oh',\n",
       " 'yeah',\n",
       " 'said']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If more than half of the comedians have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'noone', 'latter', 'done', 'thereby', 'nothing', 'anyhow', 'next', 'is', 'front', 'rather', 'otherwise', 'further', 'seemed', 'must', 'via', 'bill', 'itself', 'for', 'hence', 'there', 'what', 'three', 'or', 'whose', 'down', 'somewhere', 'afterwards', 'neither', 'beforehand', 'therefore', 'just', 'nine', 'that', 'about', 'upon', 'think', 'across', 'before', 'myself', 'although', 'often', 'she', 'ourselves', 'hers', 'almost', 'towards', 'wherever', 'many', 'whatever', 'but', 'still', 'above', 'oh', 'how', 'onto', 'first', 'mine', 'etc', 'several', 'detail', 'those', 'everything', 'and', 'perhaps', 'yet', 'off', 'yours', 'anything', 'ie', 'already', 'people', 'whole', 'empty', 'along', 'up', 'other', 'sometime', 'wherein', 'also', 'no', 'do', 'too', 'you', 'these', 'they', 'toward', 'cry', 'im', 'meanwhile', 'beside', 'even', 'i', 'at', 'sixty', 'thereafter', 'until', 'very', 'am', 'everywhere', 'except', 'out', 'something', 'one', 'third', 'thats', 'had', 'seeming', 'becomes', 'most', 'were', 'amoungst', 'each', 'when', 'twenty', 'however', 'ever', 'hereby', 'than', 'if', 'any', 'such', 'might', 'everyone', 'by', 'while', 'move', 'anyone', 'nor', 'can', 'becoming', 'our', 'eight', 'become', 'put', 'whom', 'to', 'take', 'among', 'system', 'from', 'mostly', 'show', 'alone', 'six', 'again', 'her', 'keep', 'got', 'whereupon', 'due', 'since', 'either', 'ours', 'sometimes', 'against', 'see', 'serious', 'full', 'formerly', 'therein', 'whereafter', 'after', 're', 'cant', 'said', 'yeah', 'beyond', 'amongst', 'both', 'give', 'he', 'it', 'forty', 'have', 'part', 'of', 'yourself', 'his', 'on', 'under', 'thick', 'who', 'seem', 'go', 'between', 'fire', 'me', 'whither', 'himself', 'least', 'not', 'herein', 'like', 'please', 'five', 'eg', 'moreover', 'bottom', 'into', 'others', 'herself', 'cannot', 'amount', 'may', 'made', 'been', 'inc', 'we', 'thereupon', 'behind', 'within', 'below', 'anywhere', 'elsewhere', 'side', 'mill', 'all', 'once', 'yourselves', 'latterly', 'over', 'only', 'top', 'fill', 'more', 'them', 'be', 'un', 'same', 'will', 'found', 'time', 'here', 'every', 'co', 'should', 'enough', 'hereafter', 'find', 'during', 'name', 'hundred', 'why', 'whoever', 'where', 'dont', 'without', 'whether', 'last', 'through', 'their', 'two', 'hasnt', 'themselves', 'together', 'youre', 'thus', 'per', 'know', 'con', 'hereupon', 'few', 'never', 'this', 'are', 'a', 'your', 'became', 'interest', 'anyway', 'as', 'then', 'twelve', 'us', 'seems', 'whenever', 'so', 'would', 'eleven', 'with', 'namely', 'ltd', 'whereas', 'fifty', 'him', 'none', 'another', 'throughout', 'much', 'thru', 'has', 'its', 'always', 'sincere', 'de', 'call', 'could', 'whereby', 'former', 'nobody', 'ten', 'because', 'describe', 'besides', 'fifteen', 'which', 'though', 'right', 'some', 'indeed', 'being', 'get', 'own', 'nowhere', 'around', 'now', 'gonna', 'someone', 'well', 'less', 'four', 'couldnt', 'nevertheless', 'thence', 'my', 'else', 'somehow', 'was', 'whence', 'the', 'an', 'back', 'thin', 'in'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Recreate document-term matrix\u001b[39;00m\n\u001b[0;32m     12\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstop_words)\n\u001b[1;32m---> 13\u001b[0m data_cv \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m data_stop \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_cv\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mcv\u001b[38;5;241m.\u001b[39mget_feature_names())\n\u001b[0;32m     15\u001b[0m data_stop\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m data_clean\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m constraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'noone', 'latter', 'done', 'thereby', 'nothing', 'anyhow', 'next', 'is', 'front', 'rather', 'otherwise', 'further', 'seemed', 'must', 'via', 'bill', 'itself', 'for', 'hence', 'there', 'what', 'three', 'or', 'whose', 'down', 'somewhere', 'afterwards', 'neither', 'beforehand', 'therefore', 'just', 'nine', 'that', 'about', 'upon', 'think', 'across', 'before', 'myself', 'although', 'often', 'she', 'ourselves', 'hers', 'almost', 'towards', 'wherever', 'many', 'whatever', 'but', 'still', 'above', 'oh', 'how', 'onto', 'first', 'mine', 'etc', 'several', 'detail', 'those', 'everything', 'and', 'perhaps', 'yet', 'off', 'yours', 'anything', 'ie', 'already', 'people', 'whole', 'empty', 'along', 'up', 'other', 'sometime', 'wherein', 'also', 'no', 'do', 'too', 'you', 'these', 'they', 'toward', 'cry', 'im', 'meanwhile', 'beside', 'even', 'i', 'at', 'sixty', 'thereafter', 'until', 'very', 'am', 'everywhere', 'except', 'out', 'something', 'one', 'third', 'thats', 'had', 'seeming', 'becomes', 'most', 'were', 'amoungst', 'each', 'when', 'twenty', 'however', 'ever', 'hereby', 'than', 'if', 'any', 'such', 'might', 'everyone', 'by', 'while', 'move', 'anyone', 'nor', 'can', 'becoming', 'our', 'eight', 'become', 'put', 'whom', 'to', 'take', 'among', 'system', 'from', 'mostly', 'show', 'alone', 'six', 'again', 'her', 'keep', 'got', 'whereupon', 'due', 'since', 'either', 'ours', 'sometimes', 'against', 'see', 'serious', 'full', 'formerly', 'therein', 'whereafter', 'after', 're', 'cant', 'said', 'yeah', 'beyond', 'amongst', 'both', 'give', 'he', 'it', 'forty', 'have', 'part', 'of', 'yourself', 'his', 'on', 'under', 'thick', 'who', 'seem', 'go', 'between', 'fire', 'me', 'whither', 'himself', 'least', 'not', 'herein', 'like', 'please', 'five', 'eg', 'moreover', 'bottom', 'into', 'others', 'herself', 'cannot', 'amount', 'may', 'made', 'been', 'inc', 'we', 'thereupon', 'behind', 'within', 'below', 'anywhere', 'elsewhere', 'side', 'mill', 'all', 'once', 'yourselves', 'latterly', 'over', 'only', 'top', 'fill', 'more', 'them', 'be', 'un', 'same', 'will', 'found', 'time', 'here', 'every', 'co', 'should', 'enough', 'hereafter', 'find', 'during', 'name', 'hundred', 'why', 'whoever', 'where', 'dont', 'without', 'whether', 'last', 'through', 'their', 'two', 'hasnt', 'themselves', 'together', 'youre', 'thus', 'per', 'know', 'con', 'hereupon', 'few', 'never', 'this', 'are', 'a', 'your', 'became', 'interest', 'anyway', 'as', 'then', 'twelve', 'us', 'seems', 'whenever', 'so', 'would', 'eleven', 'with', 'namely', 'ltd', 'whereas', 'fifty', 'him', 'none', 'another', 'throughout', 'much', 'thru', 'has', 'its', 'always', 'sincere', 'de', 'call', 'could', 'whereby', 'former', 'nobody', 'ten', 'because', 'describe', 'besides', 'fifteen', 'which', 'though', 'right', 'some', 'indeed', 'being', 'get', 'own', 'nowhere', 'around', 'now', 'gonna', 'someone', 'well', 'less', 'four', 'couldnt', 'nevertheless', 'thence', 'my', 'else', 'somehow', 'was', 'whence', 'the', 'an', 'back', 'thin', 'in'}) instead."
     ]
    }
   ],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n",
    "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
    "\n",
    "# Create subplots for each comedian\n",
    "for index, comedian in enumerate(data.columns):\n",
    "    wc.generate(data_clean.transcript[comedian])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_names[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ali Wong says the s-word a lot and talks about her husband. I guess that's funny to me.\n",
    "* A lot of people use the F-word. Let's dig into that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique words that each comedian uses\n",
    "\n",
    "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
    "unique_list = []\n",
    "for comedian in data.columns:\n",
    "    uniques = data[comedian].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "# Create a new dataframe that contains this unique word count\n",
    "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\n",
    "data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the words per minute of each comedian\n",
    "\n",
    "# Find the total number of words that a comedian uses\n",
    "total_list = []\n",
    "for comedian in data.columns:\n",
    "    totals = sum(data[comedian])\n",
    "    total_list.append(totals)\n",
    "    \n",
    "# Comedy special run times from IMDB, in minutes\n",
    "run_times = [60, 59, 80, 60, 67, 73, 77, 63, 62, 58, 76, 79]\n",
    "\n",
    "# Let's add some columns to our dataframe\n",
    "data_words['total_words'] = total_list\n",
    "data_words['run_times'] = run_times\n",
    "data_words['words_per_minute'] = data_words['total_words'] / data_words['run_times']\n",
    "\n",
    "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
    "data_wpm_sort = data_words.sort_values(by='words_per_minute')\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot our findings\n",
    "import numpy as np\n",
    "\n",
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
    "plt.yticks(y_pos, data_unique_sort.comedian)\n",
    "plt.title('Number of Unique Words', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')\n",
    "plt.yticks(y_pos, data_wpm_sort.comedian)\n",
    "plt.title('Number of Words Per Minute', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Vocabulary**\n",
    "   * Ricky Gervais (British comedy) and Bill Burr (podcast host) use a lot of words in their comedy\n",
    "   * Louis C.K. (self-depricating comedy) and Anthony Jeselnik (dark humor) have a smaller vocabulary\n",
    "\n",
    "\n",
    "* **Talking Speed**\n",
    "   * Joe Rogan (blue comedy) and Bill Burr (podcast host) talk fast\n",
    "   * Bo Burnham (musical comedy) and Anthony Jeselnik (dark humor) talk slow\n",
    "   \n",
    "Ali Wong is somewhere in the middle in both cases. Nothing too interesting here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of Profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier I said we'd revisit profanity. Let's take a look at the most common words again.\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just these bad words\n",
    "data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]\n",
    "data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\n",
    "data_profanity.columns = ['f_word', 's_word']\n",
    "data_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of our findings\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for i, comedian in enumerate(data_profanity.index):\n",
    "    x = data_profanity.f_word.loc[comedian]\n",
    "    y = data_profanity.s_word.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
    "    plt.xlim(-5, 155) \n",
    "    \n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of S Words', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Averaging 2 F-Bombs Per Minute!** - I don't like too much swearing, especially the f-word, which is probably why I've never heard of Bill Bur, Joe Rogan and Jim Jefferies.\n",
    "* **Clean Humor** - It looks like profanity might be a good predictor of the type of comedy I like. Besides Ali Wong, my two other favorite comedians in this group are John Mulaney and Mike Birbiglia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Side Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was our goal for the EDA portion of our journey? **To be able to take an initial look at our data and see if the results of some basic analysis made sense.**\n",
    "\n",
    "My conclusion - yes, it does, for a first pass. There are definitely some things that could be better cleaned up, such as adding more stop words or including bi-grams. But we can save that for another day. The results, especially the profanity findings, are interesting and make general sense, so we're going to move on.\n",
    "\n",
    "As a reminder, the data science process is an interative one. It's better to see some non-perfect but acceptable results to help you quickly decide whether your project is a dud or not, instead of having analysis paralysis and never delivering anything.\n",
    "\n",
    "**Alice's data science (and life) motto: Let go of perfectionism!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What other word counts do you think would be interesting to compare instead of the f-word and s-word? Create a scatter plot comparing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
